{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP4211  Tutorial5 : PyTorch (feedforward neural networks)\n",
    "\n",
    "<p style=\"text-align: left;\">\n",
    "TA: James<br \\>\n",
    "thcheungae@cse.ust.hk<br \\>\n",
    "March 23, 2019 (Mon)<br \\>\n",
    "</p>\n",
    "<url> https://hkust.zoom.us/j/570617804 </url>\n",
    "\n",
    "PyTorch\n",
    "==========\n",
    "\n",
    "Pytorch is a deep learning research platform. It is more complicated than significantly more difficult than sklearn but provides maximum flexibility and speed.\n",
    "\n",
    "Neural Network\n",
    "==========\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "  \n",
    "## Define a Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "Number of learnable parameters:\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "# install using conda:\n",
    "# conda install pytorch torchvision -c pytorch\n",
    "\n",
    "# or using pip:\n",
    "# pip install torch torchvision\n",
    "\n",
    "# or other installation, visit https://pytorch.org/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_in, d_out = 4, 2\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(x))\n",
    "        z = self.fc3(y)\n",
    "        self.fc1_out = x # for illustration\n",
    "        self.fc2_out = y\n",
    "        self.fc3_out = z\n",
    "        return z\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "print('Number of learnable parameters:')\n",
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><p>\n",
    "\n",
    "**Alternative**\n",
    "\n",
    "A faster way to build a simple neural network is the ``nn.Sequential``\n",
    "\n",
    "<pre><code>\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(d_in, 4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, d_out)\n",
    "    )\n",
    "    \n",
    "</pre></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a random input. Notice that PyTorch operates on **Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3078, -0.0892]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, d_in)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8592, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.tensor([[1.0, 0.0]])  # a dummy target, for example\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](./img/computation_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call ``loss.backward()``, the whole graph is differentiated\n",
    "w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n",
    "will have their ``.grad`` Tensor accumulated with the gradient. Now, if you follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations that looks like this:\n",
    "\n",
    "input -> linear -> relu (fc1_out) -> linear -> relu (fc2_out) -> linear (fc3_out) -> MSELoss (loss)\n",
    "      \n",
    "For illustration, let us follow a few steps backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss\n",
      "requires_grad: True\n",
      "grad_fn: <MseLossBackward object at 0x104c78828>\n",
      "\n",
      "fc3\n",
      "requires_grad: True\n",
      "grad_fn: <AddmmBackward object at 0x104c785f8>\n",
      "\n",
      "fc2\n",
      "requires_grad: True\n",
      "grad_fn: <ReluBackward0 object at 0x104c785f8>\n",
      "\n",
      "fc1\n",
      "requires_grad: True\n",
      "grad_fn: <ReluBackward0 object at 0x104c785f8>\n"
     ]
    }
   ],
   "source": [
    "for i, name in zip([loss, net.fc3_out, net.fc2_out, net.fc1_out], ['loss', 'fc3', 'fc2', 'fc1']):\n",
    "    print(f\"\\n{name}\\nrequires_grad: {i.requires_grad}\\ngrad_fn: {i.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at fc1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2.weight.grad before backward\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "fc2.weight.grad after backward\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2226, 0.0018, 0.2093, 0.0000, 0.1040, 0.0479, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, d_out)) # illustration purpose\n",
    "\n",
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('fc2.weight.grad before backward')\n",
    "print(net.fc2.weight.grad)\n",
    "\n",
    "loss.backward() # propagate loss\n",
    "\n",
    "print('fc2.weight.grad after backward')\n",
    "print(net.fc2.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the weights\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2594,  0.0925, -0.0070, -0.0130, -0.2776,  0.0488, -0.1821,  0.1503],\n",
      "        [-0.0967, -0.0249, -0.1245, -0.2778,  0.2521,  0.3157, -0.0173,  0.1020],\n",
      "        [ 0.1447, -0.0987,  0.2112,  0.0083, -0.2679, -0.0283, -0.0457,  0.0715],\n",
      "        [-0.3447,  0.0540,  0.0016,  0.1446, -0.0068, -0.2499, -0.0185, -0.2585]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2594,  0.0925, -0.0070, -0.0130, -0.2776,  0.0488, -0.1821,  0.1503],\n",
      "        [-0.1189, -0.0251, -0.1454, -0.2778,  0.2417,  0.3109, -0.0173,  0.1020],\n",
      "        [ 0.1447, -0.0987,  0.2112,  0.0083, -0.2679, -0.0283, -0.0457,  0.0715],\n",
      "        [-0.3447,  0.0540,  0.0016,  0.1446, -0.0068, -0.2499, -0.0185, -0.2585]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward() # propagate loss\n",
    "\n",
    "print(net.fc2.weight)\n",
    "optimizer.step()    # does the update w = w - eta * gradient\n",
    "print(net.fc2.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 216.07546997070312\n",
      "Epoch 1: loss: 215.9506072998047\n",
      "Epoch 2: loss: 215.8314666748047\n",
      "Epoch 3: loss: 215.71768188476562\n",
      "Epoch 4: loss: 215.60934448242188\n",
      "Epoch 5: loss: 215.50621032714844\n",
      "Epoch 6: loss: 215.40774536132812\n",
      "Epoch 7: loss: 215.3138885498047\n",
      "Epoch 8: loss: 215.22413635253906\n",
      "Epoch 9: loss: 215.13836669921875\n",
      "Epoch 10: loss: 215.05642700195312\n",
      "Epoch 11: loss: 214.97811889648438\n",
      "Epoch 12: loss: 214.90362548828125\n",
      "Epoch 13: loss: 214.8328399658203\n",
      "Epoch 14: loss: 214.76515197753906\n",
      "Epoch 15: loss: 214.70040893554688\n",
      "Epoch 16: loss: 214.63845825195312\n",
      "Epoch 17: loss: 214.57925415039062\n",
      "Epoch 18: loss: 214.5225372314453\n",
      "Epoch 19: loss: 214.4682159423828\n"
     ]
    }
   ],
   "source": [
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(100, d_in)\n",
    "y = torch.randn(100, d_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Net()\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(20):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(f'Epoch {t}: loss: {loss.item()}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "\n",
    "Dataset are not always given in a .csv format. You have to build your own PyTorch ``utils.data.Dataset`` and feed it to a then feed the data to the model using the ``utils.data.Dataloader``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attrition</th>\n",
       "      <th>Age</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>41</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>49</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>37</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>33</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>27</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Attrition  Age     BusinessTravel  DailyRate              Department  \\\n",
       "0       Yes   41      Travel_Rarely       1102                   Sales   \n",
       "1        No   49  Travel_Frequently        279  Research & Development   \n",
       "2       Yes   37      Travel_Rarely       1373  Research & Development   \n",
       "3        No   33  Travel_Frequently       1392  Research & Development   \n",
       "4        No   27      Travel_Rarely        591  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "\n",
       "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
       "0  ...                         1            80                 0   \n",
       "1  ...                         4            80                 1   \n",
       "2  ...                         2            80                 0   \n",
       "3  ...                         3            80                 0   \n",
       "4  ...                         4            80                 1   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0               1               6   \n",
       "1                 10                      3               3              10   \n",
       "2                  7                      3               3               0   \n",
       "3                  8                      3               3               8   \n",
       "4                  6                      3               3               2   \n",
       "\n",
       "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                  4                        0                     5  \n",
       "1                  7                        1                     7  \n",
       "2                  0                        0                     0  \n",
       "3                  7                        3                     0  \n",
       "4                  2                        2                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "You may download the dataset from:\n",
    "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset\n",
    "'''\n",
    "\n",
    "data_path = './HR.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HrDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.classes = [0, 1]\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Some preprocessing\n",
    "        for col in df.columns:\n",
    "            if df.dtypes[col] == \"object\":\n",
    "                df[col] = df[col].fillna(\"NA\")\n",
    "                df[col] = df[col].astype('category')\n",
    "                if len(df[col].cat.categories) > 2:\n",
    "                    df = pd.get_dummies(df, columns=[col])\n",
    "                else:\n",
    "                    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "            else:\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        X = np.array(self.df.iloc[idx, 1:]).astype(np.float32)\n",
    "        y = self.df.iloc[idx, 0]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 0, Shape: (53,), Label: 1\n",
      "[4.1000e+01 1.1020e+03 1.0000e+00 2.0000e+00 1.0000e+00 1.0000e+00\n",
      " 2.0000e+00 0.0000e+00 9.4000e+01 3.0000e+00 2.0000e+00 4.0000e+00\n",
      " 5.9930e+03 1.9479e+04 8.0000e+00 0.0000e+00 1.0000e+00 1.1000e+01\n",
      " 3.0000e+00 1.0000e+00 8.0000e+01 0.0000e+00 8.0000e+00 0.0000e+00\n",
      " 1.0000e+00 6.0000e+00 4.0000e+00 0.0000e+00 5.0000e+00 0.0000e+00\n",
      " 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00\n",
      " 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00]\n",
      "Data 1, Shape: (53,), Label: 0\n",
      "[4.9000e+01 2.7900e+02 8.0000e+00 1.0000e+00 1.0000e+00 2.0000e+00\n",
      " 3.0000e+00 1.0000e+00 6.1000e+01 2.0000e+00 2.0000e+00 2.0000e+00\n",
      " 5.1300e+03 2.4907e+04 1.0000e+00 0.0000e+00 0.0000e+00 2.3000e+01\n",
      " 4.0000e+00 4.0000e+00 8.0000e+01 1.0000e+00 1.0000e+01 3.0000e+00\n",
      " 3.0000e+00 1.0000e+01 7.0000e+00 1.0000e+00 7.0000e+00 0.0000e+00\n",
      " 1.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00\n",
      " 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00]\n"
     ]
    }
   ],
   "source": [
    "data = HrDataset(file_path=data_path)\n",
    "\n",
    "classes = data.classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    features, label = data[i]\n",
    "    print(f'Data {i}, Shape: {features.shape}, Label: {label}')\n",
    "    print(features)\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full:\t\t1470\n",
      "|Train:\t\t940\n",
      "|Validation:\t236\n",
      "|Test:\t\t294\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "full_data = HrDataset(file_path=data_path)\n",
    "\n",
    "train_size = int(0.8 * len(full_data))\n",
    "test_size = len(full_data) - train_size\n",
    "\n",
    "full_train, test = random_split(full_data, [train_size, test_size])\n",
    "\n",
    "train_size = int(0.8 * len(full_train))\n",
    "valid_size = len(full_train) - train_size\n",
    "\n",
    "train, valid = random_split(full_train, [train_size, valid_size])\n",
    "\n",
    "print(f'Full:\\t\\t{len(full_data)}')\n",
    "print(f'|Train:\\t\\t{len(train)}')\n",
    "print(f'|Validation:\\t{len(valid)}')\n",
    "print(f'|Test:\\t\\t{len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader Class\n",
    "\n",
    "However, we are losing a lot of features by using a simple for loop to iterate over the data. In particular, we are missing functions like **Batching** the data, **Shuffling** the data and Load the data in parallel using multiprocessing workers.\n",
    "\n",
    "``torch.utils.data.DataLoader`` is an iterator which provides all these features. Parameters used below should be clear. One parameter of interest is collate_fn. You can specify how exactly the samples need to be batched using ``collate_fn``. However, default collate should work fine for most use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched features:\n",
      " tensor([[5.1000e+01, 4.3200e+02, 9.0000e+00,  ..., 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.8000e+01, 4.4000e+02, 2.1000e+01,  ..., 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.2000e+01, 5.9400e+02, 2.0000e+00,  ..., 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [2.6000e+01, 4.2600e+02, 1.7000e+01,  ..., 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [3.0000e+01, 1.3120e+03, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00],\n",
      "        [3.1000e+01, 6.6700e+02, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00]]),       \n",
      " Batched labels:\n",
      " tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=1)\n",
    "\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=1)\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=1,\n",
    "                         shuffle=True, num_workers=1)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "inputs, labels = dataiter.next()\n",
    "\n",
    "print(f'Batched features:\\n {inputs}, \\\n",
    "      \\n Batched labels:\\n {labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /Users/tszhimcheung/Virtualenvs/nlp_environment/lib/python3.6/site-packages (1.5.1)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1                   [-1, 53]             106\n",
      "            Linear-2                  [-1, 128]           6,912\n",
      "           Dropout-3                  [-1, 128]               0\n",
      "       BatchNorm1d-4                  [-1, 128]             256\n",
      "            Linear-5                   [-1, 64]           8,256\n",
      "           Dropout-6                   [-1, 64]               0\n",
      "       BatchNorm1d-7                   [-1, 64]             128\n",
      "            Linear-8                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 15,788\n",
      "Trainable params: 15,788\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchsummary is not available in conda install, \n",
    "# you need to install using pip\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "d_in, d_out = 53, 2\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, d_out)\n",
    "        self.bn1 = nn.BatchNorm1d(d_in)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "## gpu or cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = Net().to(device)\n",
    "\n",
    "summary(net, input_size=(53,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "Tensorboard is a powerful platform to visualize the training performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/hr_experiment')\n",
    "\n",
    "# add graph\n",
    "writer.add_graph(net, inputs)\n",
    "writer.close()\n",
    "\n",
    "# run tensorboard --logdir='./runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Validation - the model.fit( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [20/1500], Train Loss: 0.6942, Valid Loss: 0.4005\n",
      "Epoch [2/50], Step [40/1500], Train Loss: 0.6162, Valid Loss: 0.3702\n",
      "Epoch [2/50], Step [60/1500], Train Loss: 0.5634, Valid Loss: 0.4647\n",
      "Epoch [3/50], Step [80/1500], Train Loss: 0.4985, Valid Loss: 0.4567\n",
      "Epoch [4/50], Step [100/1500], Train Loss: 0.4550, Valid Loss: 0.4200\n",
      "Epoch [4/50], Step [120/1500], Train Loss: 0.4633, Valid Loss: 0.3843\n",
      "Epoch [5/50], Step [140/1500], Train Loss: 0.4029, Valid Loss: 0.3580\n",
      "Epoch [6/50], Step [160/1500], Train Loss: 0.3955, Valid Loss: 0.3401\n",
      "Epoch [6/50], Step [180/1500], Train Loss: 0.3455, Valid Loss: 0.2992\n",
      "Epoch [7/50], Step [200/1500], Train Loss: 0.3253, Valid Loss: 0.2655\n",
      "Epoch [8/50], Step [220/1500], Train Loss: 0.3242, Valid Loss: 0.2861\n",
      "Epoch [8/50], Step [240/1500], Train Loss: 0.3214, Valid Loss: 0.2564\n",
      "Epoch [9/50], Step [260/1500], Train Loss: 0.3013, Valid Loss: 0.2645\n",
      "Epoch [10/50], Step [280/1500], Train Loss: 0.3083, Valid Loss: 0.2882\n",
      "Epoch [10/50], Step [300/1500], Train Loss: 0.2794, Valid Loss: 0.2433\n",
      "Epoch [11/50], Step [320/1500], Train Loss: 0.2907, Valid Loss: 0.2481\n",
      "Epoch [12/50], Step [340/1500], Train Loss: 0.3008, Valid Loss: 0.2629\n",
      "Epoch [12/50], Step [360/1500], Train Loss: 0.2468, Valid Loss: 0.2590\n",
      "Epoch [13/50], Step [380/1500], Train Loss: 0.2488, Valid Loss: 0.2658\n",
      "Epoch [14/50], Step [400/1500], Train Loss: 0.2632, Valid Loss: 0.2579\n",
      "Epoch [14/50], Step [420/1500], Train Loss: 0.2748, Valid Loss: 0.2665\n",
      "Epoch [15/50], Step [440/1500], Train Loss: 0.2592, Valid Loss: 0.2596\n",
      "Epoch [16/50], Step [460/1500], Train Loss: 0.2605, Valid Loss: 0.2766\n",
      "Epoch [16/50], Step [480/1500], Train Loss: 0.2600, Valid Loss: 0.2752\n",
      "Epoch [17/50], Step [500/1500], Train Loss: 0.2419, Valid Loss: 0.2610\n",
      "Epoch [18/50], Step [520/1500], Train Loss: 0.2678, Valid Loss: 0.2632\n",
      "Epoch [18/50], Step [540/1500], Train Loss: 0.2311, Valid Loss: 0.2518\n",
      "Epoch [19/50], Step [560/1500], Train Loss: 0.2094, Valid Loss: 0.2651\n",
      "Epoch [20/50], Step [580/1500], Train Loss: 0.2339, Valid Loss: 0.2626\n",
      "Epoch [20/50], Step [600/1500], Train Loss: 0.2003, Valid Loss: 0.2570\n",
      "Epoch [21/50], Step [620/1500], Train Loss: 0.2164, Valid Loss: 0.2700\n",
      "Epoch [22/50], Step [640/1500], Train Loss: 0.2014, Valid Loss: 0.3010\n",
      "Epoch [22/50], Step [660/1500], Train Loss: 0.2009, Valid Loss: 0.3174\n",
      "Epoch [23/50], Step [680/1500], Train Loss: 0.1856, Valid Loss: 0.2764\n",
      "Epoch [24/50], Step [700/1500], Train Loss: 0.2171, Valid Loss: 0.2771\n",
      "Epoch [24/50], Step [720/1500], Train Loss: 0.1943, Valid Loss: 0.2687\n",
      "Epoch [25/50], Step [740/1500], Train Loss: 0.1709, Valid Loss: 0.2804\n",
      "Epoch [26/50], Step [760/1500], Train Loss: 0.1980, Valid Loss: 0.3008\n",
      "Epoch [26/50], Step [780/1500], Train Loss: 0.1828, Valid Loss: 0.2871\n",
      "Epoch [27/50], Step [800/1500], Train Loss: 0.1796, Valid Loss: 0.3081\n",
      "Epoch [28/50], Step [820/1500], Train Loss: 0.1643, Valid Loss: 0.3042\n",
      "Epoch [28/50], Step [840/1500], Train Loss: 0.2114, Valid Loss: 0.2947\n",
      "Epoch [29/50], Step [860/1500], Train Loss: 0.1478, Valid Loss: 0.3330\n",
      "Epoch [30/50], Step [880/1500], Train Loss: 0.2572, Valid Loss: 0.2896\n",
      "Epoch [30/50], Step [900/1500], Train Loss: 0.1965, Valid Loss: 0.3268\n",
      "Epoch [31/50], Step [920/1500], Train Loss: 0.1663, Valid Loss: 0.3263\n",
      "Epoch [32/50], Step [940/1500], Train Loss: 0.1759, Valid Loss: 0.3048\n",
      "Epoch [32/50], Step [960/1500], Train Loss: 0.1699, Valid Loss: 0.2938\n",
      "Epoch [33/50], Step [980/1500], Train Loss: 0.1635, Valid Loss: 0.2883\n",
      "Epoch [34/50], Step [1000/1500], Train Loss: 0.1610, Valid Loss: 0.3042\n",
      "Epoch [34/50], Step [1020/1500], Train Loss: 0.1750, Valid Loss: 0.3068\n",
      "Epoch [35/50], Step [1040/1500], Train Loss: 0.1433, Valid Loss: 0.3191\n",
      "Epoch [36/50], Step [1060/1500], Train Loss: 0.1518, Valid Loss: 0.3329\n",
      "Epoch [36/50], Step [1080/1500], Train Loss: 0.1251, Valid Loss: 0.3229\n",
      "Epoch [37/50], Step [1100/1500], Train Loss: 0.1685, Valid Loss: 0.2940\n",
      "Epoch [38/50], Step [1120/1500], Train Loss: 0.1212, Valid Loss: 0.3103\n",
      "Epoch [38/50], Step [1140/1500], Train Loss: 0.1290, Valid Loss: 0.3684\n",
      "Epoch [39/50], Step [1160/1500], Train Loss: 0.1396, Valid Loss: 0.3141\n",
      "Epoch [40/50], Step [1180/1500], Train Loss: 0.1438, Valid Loss: 0.3236\n",
      "Epoch [40/50], Step [1200/1500], Train Loss: 0.1303, Valid Loss: 0.3473\n",
      "Epoch [41/50], Step [1220/1500], Train Loss: 0.1375, Valid Loss: 0.3919\n",
      "Epoch [42/50], Step [1240/1500], Train Loss: 0.1507, Valid Loss: 0.3277\n",
      "Epoch [42/50], Step [1260/1500], Train Loss: 0.1497, Valid Loss: 0.3633\n",
      "Epoch [43/50], Step [1280/1500], Train Loss: 0.1596, Valid Loss: 0.3707\n",
      "Epoch [44/50], Step [1300/1500], Train Loss: 0.1225, Valid Loss: 0.3170\n",
      "Epoch [44/50], Step [1320/1500], Train Loss: 0.1358, Valid Loss: 0.3161\n",
      "Epoch [45/50], Step [1340/1500], Train Loss: 0.1237, Valid Loss: 0.3939\n",
      "Epoch [46/50], Step [1360/1500], Train Loss: 0.1689, Valid Loss: 0.3292\n",
      "Epoch [46/50], Step [1380/1500], Train Loss: 0.1328, Valid Loss: 0.3270\n",
      "Epoch [47/50], Step [1400/1500], Train Loss: 0.1036, Valid Loss: 0.3173\n",
      "Epoch [48/50], Step [1420/1500], Train Loss: 0.1300, Valid Loss: 0.3463\n",
      "Epoch [48/50], Step [1440/1500], Train Loss: 0.1394, Valid Loss: 0.3482\n",
      "Epoch [49/50], Step [1460/1500], Train Loss: 0.1229, Valid Loss: 0.3565\n",
      "Epoch [50/50], Step [1480/1500], Train Loss: 0.1336, Valid Loss: 0.3604\n",
      "Epoch [50/50], Step [1500/1500], Train Loss: 0.1362, Valid Loss: 0.3757\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "global_step = 0\n",
    "eval_every = 20\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "total_step = len(train_loader)*num_epochs\n",
    "\n",
    "net = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        net.train()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        '''Training of the model'''\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        '''Evaluating the model every x steps'''\n",
    "        if global_step % eval_every == 0:\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                val_running_loss = 0.0\n",
    "                for val_inputs, val_labels in valid_loader:\n",
    "                    val_outputs = net(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    val_running_loss += val_loss.item()\n",
    "\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_val_loss = val_running_loss / len(valid_loader)\n",
    "\n",
    "                # ...log the running loss\n",
    "                writer.add_scalar(\n",
    "                    f'training loss {num_epochs}', average_train_loss, global_step)\n",
    "\n",
    "                # ...log the running loss\n",
    "                writer.add_scalar(\n",
    "                    f'validation loss {num_epochs}', average_val_loss, global_step)\n",
    "\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, total_step, average_train_loss, average_val_loss))\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       245\n",
      "           1       0.56      0.47      0.51        49\n",
      "\n",
      "    accuracy                           0.85       294\n",
      "   macro avg       0.73      0.70      0.71       294\n",
      "weighted avg       0.84      0.85      0.84       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def eval(model, test_loader):\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            model.eval()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.append(predicted.item())\n",
    "            y_test.append(labels.item())\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "eval(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(ckpt_dir, model, optimizer, epoch):\n",
    "    save_path = f'{ckpt_dir}_{epoch}.pt'\n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'epoch': epoch}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "    return save_path\n",
    "\n",
    "\n",
    "def load_checkpoint(save_path, model, optimizer):\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "\n",
    "    print(f'Model loaded from <== {save_path}')\n",
    "\n",
    "    return state_dict['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> ./hr_model_10.pt\n",
      "Model loaded from <== ./hr_model_10.pt\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       245\n",
      "           1       0.56      0.47      0.51        49\n",
      "\n",
      "    accuracy                           0.85       294\n",
      "   macro avg       0.73      0.70      0.71       294\n",
      "weighted avg       0.84      0.85      0.84       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = save_checkpoint('./hr_model', net, optimizer, 10)\n",
    "new_net = Net().to(device)\n",
    "last_epoch = load_checkpoint(save_path, new_net, optimizer)\n",
    "eval(new_net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><p>\n",
    "    \n",
    "**Exercise:**\n",
    "    Try to improve the performance by changing:\n",
    "  -  Number of layers\n",
    "  -  Number of hidden units\n",
    "  -  Batch Size\n",
    "  -  Learning Rate\n",
    "  -  Optimizer\n",
    "  -  Number of Epochs\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "- <url>https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset</url>\n",
    "- <url>https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</url>\n",
    "- <http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds02.pdf>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
